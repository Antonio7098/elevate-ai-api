"""
Google Gemini API service for LLM response generation.

This module provides the GeminiService class for interacting with Google's Gemini API
for generating responses in the RAG chat system.
"""

import os
import asyncio
import logging
from typing import Optional, Dict, Any, List
from dataclasses import dataclass

try:
    import google.generativeai as genai
except ImportError:
    genai = None

logger = logging.getLogger(__name__)


@dataclass
class GeminiConfig:
    """Configuration for Gemini API service."""
    api_key: str
    model_name: str = "gemini-1.5-flash"
    temperature: float = 0.7
    max_tokens: int = 1000
    top_p: float = 0.9
    top_k: int = 40


class GeminiService:
    """Service for interacting with Google Gemini API."""
    
    def __init__(self, config: Optional[GeminiConfig] = None):
        """Initialize Gemini service with configuration."""
        if config is None:
            # Use environment variables for configuration
            api_key = os.getenv("GOOGLE_API_KEY")
            if not api_key:
                logger.warning("GOOGLE_API_KEY not found, using mock mode")
                config = GeminiConfig(api_key="mock_key")
            else:
                config = GeminiConfig(api_key=api_key)
        
        self.config = config
        self.model = None
        
        # Check if google.generativeai is available
        if genai is None:
            logger.warning("google.generativeai not available. Using mock responses.")
            self.mock_mode = True
        else:
            self.mock_mode = False
            # Configure the API
            genai.configure(api_key=self.config.api_key)
            self.model = genai.GenerativeModel(self.config.model_name)
    
    async def generate_response(
        self,
        prompt: str,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        **kwargs
    ) -> str:
        """
        Generate a response using Gemini API.
        
        Args:
            prompt: The input prompt for generation
            max_tokens: Maximum number of tokens to generate
            temperature: Temperature for response generation
            **kwargs: Additional generation parameters
            
        Returns:
            Generated response text
        """
        try:
            # Use provided parameters or fall back to config defaults
            max_tokens = max_tokens or self.config.max_tokens
            temperature = temperature or self.config.temperature
            
            if self.mock_mode:
                # Return mock response for testing
                return await self._generate_mock_response(prompt)
            
            # Generate response using Gemini API
            generation_config = genai.types.GenerationConfig(
                temperature=temperature,
                max_output_tokens=max_tokens,
                top_p=self.config.top_p,
                top_k=self.config.top_k
            )
            
            # Run the API call in a thread to avoid blocking
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.model.generate_content(
                    prompt,
                    generation_config=generation_config
                )
            )
            
            if response.candidates and len(response.candidates) > 0:
                return response.candidates[0].content.parts[0].text
            else:
                raise ValueError("No response generated by Gemini API")
                
        except Exception as e:
            logger.error(f"Error generating response with Gemini: {e}")
            # Return fallback response
            return await self._generate_fallback_response(prompt)
    
    async def _generate_mock_response(self, prompt: str) -> str:
        """Generate mock response for testing."""
        # Simple mock response based on prompt content
        if "python" in prompt.lower():
            return "Python is a versatile programming language known for its simplicity and readability. It's widely used for web development, data science, machine learning, and automation tasks."
        elif "machine learning" in prompt.lower():
            return "Machine learning is a subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed. It uses algorithms to identify patterns in data."
        elif "function" in prompt.lower():
            return "Functions in Python are reusable blocks of code that perform specific tasks. They are defined using the 'def' keyword followed by the function name and parameters in parentheses."
        elif "variable" in prompt.lower():
            return "Variables in Python are used to store data values. They are created by assignment and don't need explicit declaration. Python automatically determines the variable type based on the assigned value."
        elif "web development" in prompt.lower():
            return "Web development involves creating websites and web applications. It typically includes front-end development (HTML, CSS, JavaScript) and back-end development (server-side programming, databases)."
        else:
            return "I'm here to help you learn and understand various programming concepts. Please feel free to ask me specific questions about programming, and I'll provide detailed explanations and examples."
    
    async def _generate_fallback_response(self, prompt: str) -> str:
        """Generate fallback response when API fails."""
        return "I'm having trouble generating a response right now. Please try rephrasing your question or check back later."
    
    async def embed_text(self, text: str) -> List[float]:
        """
        Generate embeddings for text using Gemini API.
        
        Args:
            text: Text to embed
            
        Returns:
            List of embedding values
        """
        try:
            if self.mock_mode:
                # Return mock embedding for testing
                import hashlib
                # Generate consistent mock embedding based on text hash
                text_hash = hashlib.md5(text.encode()).hexdigest()
                # Convert hash to float values between -1 and 1
                embedding = []
                for i in range(0, len(text_hash), 8):
                    chunk = text_hash[i:i+8]
                    value = int(chunk, 16) / (16**8) * 2 - 1
                    embedding.append(value)
                
                # Pad or truncate to standard embedding size (384 for testing)
                target_size = 384
                if len(embedding) < target_size:
                    embedding.extend([0.0] * (target_size - len(embedding)))
                else:
                    embedding = embedding[:target_size]
                
                return embedding
            
            # Use Gemini API for embeddings
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None,
                lambda: genai.embed_content(
                    model="models/embedding-001",
                    content=text
                )
            )
            
            return result["embedding"]
            
        except Exception as e:
            logger.error(f"Error generating embeddings with Gemini: {e}")
            # Return default embedding
            return [0.0] * 384
    
    def is_available(self) -> bool:
        """Check if Gemini API is available."""
        return not self.mock_mode
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get information about the current model."""
        return {
            "model_name": self.config.model_name,
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens,
            "mock_mode": self.mock_mode
        }


# Convenience function for creating GeminiService instance
def create_gemini_service(api_key: Optional[str] = None) -> GeminiService:
    """Create a GeminiService instance with default configuration."""
    if api_key:
        config = GeminiConfig(api_key=api_key)
        return GeminiService(config)
    else:
        return GeminiService()
