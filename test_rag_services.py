#!/usr/bin/env python3
"""
Production readiness test for RAG & Search Services.
Tests RAG engine, GraphRAG, search, and knowledge retrieval with REAL LLM calls.
"""

import asyncio
import os
import sys
import uuid
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Add the app directory to the Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'app'))

from app.services.llm_service import create_llm_service
from app.core.vector_store import PineconeVectorStore, ChromaDBVectorStore
from app.core.embeddings import GoogleEmbeddingService
from app.core.rag_engine import RAGEngine
from app.core.rag_search import RAGSearchService
from app.core.search_service import SearchService
from app.core.indexing_pipeline import IndexingPipeline
from app.core.metadata_indexing import MetadataIndexingService

class RAGServicesTester:
    def __init__(self):
        self.llm_service = None
        self.vector_store = None
        self.embedding_service = None
        self.rag_engine = None
        self.rag_search_service = None
        self.search_service = None
        self.indexing_pipeline = None
        self.metadata_indexing = None
        
    async def setup_services(self):
        """Set up all RAG services with real dependencies."""
        print("ğŸ”§ Setting up RAG & Search Services...")
        
        try:
            # Set up LLM service
            print("   ğŸš€ Setting up Gemini LLM service...")
            self.llm_service = create_llm_service(provider="gemini")
            print("   âœ… LLM service ready")
            
            # Set up Vector Store (Pinecone or ChromaDB)
            print("   ğŸŒ² Setting up vector store...")
            pinecone_api_key = os.getenv("PINECONE_API_KEY")
            pinecone_env = os.getenv("PINECONE_ENVIRONMENT")
            
            if pinecone_api_key and pinecone_env:
                self.vector_store = PineconeVectorStore(
                    api_key=pinecone_api_key, 
                    environment=pinecone_env
                )
                await self.vector_store.initialize()
                print("   âœ… Pinecone store ready")
            else:
                self.vector_store = ChromaDBVectorStore(persist_directory="./test_chroma")
                await self.vector_store.initialize()
                print("   âœ… ChromaDB store ready")
            
            # Set up Embedding Service
            print("   ğŸ”¤ Setting up Google embedding service...")
            google_api_key = os.getenv("GOOGLE_API_KEY")
            if not google_api_key:
                raise Exception("Missing Google API key for embeddings")
                
            self.embedding_service = GoogleEmbeddingService(api_key=google_api_key)
            print("   âœ… Embedding service ready")
            
            # Set up RAG Engine
            print("   ğŸ” Setting up RAG Engine...")
            self.rag_engine = RAGEngine(
                vector_store=self.vector_store,
                embedding_service=self.embedding_service
            )
            print("   âœ… RAG Engine ready")
            
            # Set up RAG Search Service
            print("   ğŸ” Setting up RAG Search Service...")
            self.rag_search_service = RAGSearchService(
                vector_store=self.vector_store,
                embedding_service=self.embedding_service
            )
            print("   âœ… RAG Search Service ready")
            
            # Set up Search Service
            print("   ğŸ” Setting up Search Service...")
            self.search_service = SearchService(
                vector_store=self.vector_store,
                embedding_service=self.embedding_service
            )
            print("   âœ… Search Service ready")
            
            # Set up Indexing Pipeline
            print("   ğŸ“¥ Setting up Indexing Pipeline...")
            self.indexing_pipeline = IndexingPipeline()
            print("   âœ… Indexing Pipeline ready")
            
            # Set up Metadata Indexing
            print("   ğŸ·ï¸  Setting up Metadata Indexing...")
            self.metadata_indexing = MetadataIndexingService()
            print("   âœ… Metadata Indexing ready")
            
            print("   ğŸ‰ All RAG & Search Services set up successfully!")
            return True
            
        except Exception as e:
            print(f"   âŒ Service setup failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    async def test_rag_engine(self):
        """Test RAG engine with real LLM calls."""
        print("\nğŸ” Testing RAG Engine")
        print("-" * 60)
        
        try:
            print("   ğŸš€ Testing RAG engine...")
            
            # Create test index for RAG
            test_index = f"rag-engine-test-{uuid.uuid4().hex[:8]}"
            print(f"      ğŸ—ï¸  Creating test index: {test_index}")
            
            await self.vector_store.create_index(test_index, dimension=768)
            
            # Add test content for RAG
            test_content = [
                {
                    "id": "rag-1",
                    "content": "Machine learning is a subset of artificial intelligence that enables computers to learn from data without being explicitly programmed.",
                    "metadata": {"source": "ai_basics", "topic": "machine_learning"}
                },
                {
                    "id": "rag-2",
                    "content": "Deep learning uses neural networks with multiple layers to model complex patterns in data.",
                    "metadata": {"source": "ai_advanced", "topic": "deep_learning"}
                },
                {
                    "id": "rag-3",
                    "content": "Supervised learning involves training a model on labeled data to make predictions.",
                    "metadata": {"source": "ml_fundamentals", "topic": "supervised_learning"}
                }
            ]
            
            # Generate embeddings and index
            print("      ğŸ”¤ Generating embeddings for RAG content...")
            texts = [item["content"] for item in test_content]
            embeddings = await self.embedding_service.embed_batch(texts)
            
            vectors = []
            for content, embedding in zip(test_content, embeddings):
                vectors.append({
                    "id": content["id"],
                    "values": embedding,
                    "metadata": content["metadata"]
                })
            
            await self.vector_store.upsert_vectors(test_index, vectors)
            print("      âœ… RAG content indexed")
            
            # Wait for indexing
            await asyncio.sleep(3)
            
            # Test RAG query generation
            print("      ğŸ” Testing RAG query generation...")
            query = "What is machine learning and how does it relate to deep learning?"
            
            try:
                # Test RAG engine response generation
                rag_response = await self.rag_engine.generate_answer(
                    query=query,
                    context="Machine learning enables computers to learn from data. Deep learning uses neural networks."
                )
                
                print(f"         âœ… RAG response generated: {len(rag_response.get('answer', ''))} characters")
                
                if rag_response:
                    print("      ğŸ“Š RAG response details:")
                    print(f"         Answer: {rag_response.get('answer', 'N/A')[:100]}...")
                    print(f"         Confidence: {rag_response.get('confidence', 'N/A')}")
                    print(f"         Sources: {rag_response.get('sources', 'N/A')}")
                    
            except Exception as e:
                print(f"         âš ï¸  RAG engine response generation failed: {e}")
            
            # Test RAG context retrieval
            print("      ğŸ“š Testing RAG context retrieval...")
            try:
                context = await self.rag_engine.retrieve_context(
                    query=query,
                    max_results=3
                )
                
                print(f"         âœ… Context retrieved: {len(context)} characters")
                
            except Exception as e:
                print(f"         âš ï¸  RAG context retrieval failed: {e}")
            
            # Cleanup
            await self.vector_store.delete_index(test_index)
            print("      âœ… Test index deleted")
            
            print("   âœ… RAG engine test completed")
            return True
            
        except Exception as e:
            print(f"   âŒ RAG engine test failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    async def test_rag_search_service(self):
        """Test RAG search service with real functionality."""
        print("\nğŸ” Testing RAG Search Service")
        print("-" * 60)
        
        try:
            print("   ğŸš€ Testing RAG search service...")
            
            # Create test index for RAG search
            test_index = f"rag-search-test-{uuid.uuid4().hex[:8]}"
            print(f"      ğŸ—ï¸  Creating test index: {test_index}")
            
            await self.vector_store.create_index(test_index, dimension=768)
            
            # Add comprehensive test content
            test_content = [
                {
                    "id": "search-1",
                    "content": "Linear regression is a supervised learning algorithm that predicts continuous values.",
                    "metadata": {"source": "ml_algorithms", "topic": "linear_regression", "difficulty": "basic"}
                },
                {
                    "id": "search-2",
                    "content": "Logistic regression is used for binary classification problems in machine learning.",
                    "metadata": {"source": "ml_algorithms", "topic": "logistic_regression", "difficulty": "basic"}
                },
                {
                    "id": "search-3",
                    "content": "Neural networks are computational models inspired by biological neurons.",
                    "metadata": {"source": "deep_learning", "topic": "neural_networks", "difficulty": "intermediate"}
                },
                {
                    "id": "search-4",
                    "content": "Convolutional neural networks excel at image recognition tasks.",
                    "metadata": {"source": "deep_learning", "topic": "cnn", "difficulty": "advanced"}
                }
            ]
            
            # Generate embeddings and index
            print("      ğŸ”¤ Generating embeddings for search content...")
            texts = [item["content"] for item in test_content]
            embeddings = await self.embedding_service.embed_batch(texts)
            
            vectors = []
            for content, embedding in zip(test_content, embeddings):
                vectors.append({
                    "id": content["id"],
                    "values": embedding,
                    "metadata": content["metadata"]
                })
            
            await self.vector_store.upsert_vectors(test_index, vectors)
            print("      âœ… Search content indexed")
            
            # Wait for indexing
            await asyncio.sleep(3)
            
            # Test RAG search queries
            test_queries = [
                "What is linear regression?",
                "How do neural networks work?",
                "Explain convolutional neural networks"
            ]
            
            for i, query in enumerate(test_queries, 1):
                print(f"      ğŸ” Testing RAG search query {i}: {query}")
                
                try:
                    search_results = await self.rag_search_service.search(
                        query=query,
                        blueprint_id=None,
                        max_results=3
                    )
                    
                    if search_results and "results" in search_results:
                        print(f"         âœ… RAG search returned {len(search_results['results'])} results")
                        
                        # Show first result details
                        if search_results['results']:
                            first_result = search_results['results'][0]
                            print(f"         ğŸ“Š First result: {first_result.get('content', 'N/A')[:80]}...")
                    else:
                        print(f"         âš ï¸  RAG search returned unexpected format")
                        
                except Exception as e:
                    print(f"         âš ï¸  RAG search failed: {e}")
            
            # Cleanup
            await self.vector_store.delete_index(test_index)
            print("      âœ… Test index deleted")
            
            print("   âœ… RAG search service test completed")
            return True
            
        except Exception as e:
            print(f"   âŒ RAG search service test failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    async def test_search_service(self):
        """Test search service with real functionality."""
        print("\nğŸ” Testing Search Service")
        print("-" * 60)
        
        try:
            print("   ğŸš€ Testing search service...")
            
            # Create test index for search
            test_index = f"search-test-{uuid.uuid4().hex[:8]}"
            print(f"      ğŸ—ï¸  Creating test index: {test_index}")
            
            await self.vector_store.create_index(test_index, dimension=768)
            
            # Add test content
            test_content = [
                {
                    "id": "search-1",
                    "content": "Machine learning algorithms learn patterns from data",
                    "metadata": {"category": "ml_basics", "topic": "algorithms"}
                },
                {
                    "id": "search-2",
                    "content": "Deep learning uses neural networks for complex tasks",
                    "metadata": {"category": "deep_learning", "topic": "neural_networks"}
                }
            ]
            
            # Generate embeddings and index
            print("      ğŸ”¤ Generating embeddings for search content...")
            texts = [item["content"] for item in test_content]
            embeddings = await self.embedding_service.embed_batch(texts)
            
            vectors = []
            for content, embedding in zip(test_content, embeddings):
                vectors.append({
                    "id": content["id"],
                    "values": embedding,
                    "metadata": content["metadata"]
                })
            
            await self.vector_store.upsert_vectors(test_index, vectors)
            print("      âœ… Search content indexed")
            
            # Wait for indexing
            await asyncio.sleep(3)
            
            # Test search functionality
            print("      ğŸ” Testing search functionality...")
            query = "machine learning algorithms"
            
            try:
                search_results = await self.search_service.search_nodes(
                    request={
                        "query": query,
                        "top_k": 3,
                        "filter_metadata": {"category": "ml_basics"}
                    }
                )
                
                print(f"         âœ… Search returned {len(search_results.results) if hasattr(search_results, 'results') else 'unknown'} results")
                
            except Exception as e:
                print(f"         âš ï¸  Search failed: {e}")
            
            # Cleanup
            await self.vector_store.delete_index(test_index)
            print("      âœ… Test index deleted")
            
            print("   âœ… Search service test completed")
            return True
            
        except Exception as e:
            print(f"   âŒ Search service test failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    async def test_graph_rag(self):
        """Test GraphRAG functionality with relationship-based content."""
        print("\nğŸ•¸ï¸  Testing GraphRAG")
        print("-" * 60)
        
        try:
            print("   ğŸš€ Testing GraphRAG functionality...")
            
            # Create test index for GraphRAG
            test_index = f"graphrag-test-{uuid.uuid4().hex[:8]}"
            print(f"      ğŸ—ï¸  Creating test index: {test_index}")
            
            await self.vector_store.create_index(test_index, dimension=768)
            
            # Add content with explicit relationships for GraphRAG
            graphrag_content = [
                {
                    "id": "graph-1",
                    "content": "Linear regression predicts continuous values using a linear relationship between features and target.",
                    "metadata": {
                        "source": "ml_algorithms",
                        "topic": "linear_regression",
                        "relationships": ["logistic_regression", "polynomial_regression"],
                        "algorithm_type": "supervised",
                        "output_type": "continuous"
                    }
                },
                {
                    "id": "graph-2",
                    "content": "Logistic regression extends linear regression concepts to binary classification problems.",
                    "metadata": {
                        "source": "ml_algorithms",
                        "topic": "logistic_regression",
                        "relationships": ["linear_regression", "classification"],
                        "algorithm_type": "supervised",
                        "output_type": "binary"
                    }
                },
                {
                    "id": "graph-3",
                    "content": "Neural networks form the foundation of deep learning with interconnected nodes.",
                    "metadata": {
                        "source": "deep_learning",
                        "topic": "neural_networks",
                        "relationships": ["deep_learning", "layers", "nodes"],
                        "algorithm_type": "unsupervised",
                        "architecture": "layered"
                    }
                }
            ]
            
            # Generate embeddings and index
            print("      ğŸ”¤ Generating embeddings for GraphRAG content...")
            texts = [item["content"] for item in graphrag_content]
            embeddings = await self.embedding_service.embed_batch(texts)
            
            vectors = []
            for content, embedding in zip(graphrag_content, embeddings):
                vectors.append({
                    "id": content["id"],
                    "values": embedding,
                    "metadata": content["metadata"]
                })
            
            await self.vector_store.upsert_vectors(test_index, vectors)
            print("      âœ… GraphRAG content indexed")
            
            # Wait for indexing
            await asyncio.sleep(3)
            
            # Test GraphRAG-style search with relationship awareness
            print("      ğŸ” Testing GraphRAG relationship search...")
            
            # Search for related concepts
            query = "supervised learning algorithms"
            query_embedding = await self.embedding_service.embed_text(query)
            
            # Search with relationship filtering
            supervised_results = await self.vector_store.search(
                test_index,
                query_vector=query_embedding,
                top_k=5,
                filter_metadata={"algorithm_type": "supervised"}
            )
            print(f"         âœ… Supervised algorithm search returned {len(supervised_results)} results")
            
            # Search for related topics
            regression_query = "regression algorithms"
            regression_embedding = await self.embedding_service.embed_text(regression_query)
            
            regression_results = await self.vector_store.search(
                test_index,
                query_vector=regression_embedding,
                top_k=5
            )
            print(f"         âœ… Regression search returned {len(regression_results)} results")
            
            # Test relationship-based retrieval
            print("      ğŸ”— Testing relationship-based retrieval...")
            for result in regression_results[:2]:
                if hasattr(result, 'metadata') and "relationships" in result.metadata:
                    print(f"         ğŸ“ {result.id}: {result.metadata['relationships']}")
            
            # Cleanup
            await self.vector_store.delete_index(test_index)
            print("      âœ… Test index deleted")
            
            print("   âœ… GraphRAG test completed")
            return True
            
        except Exception as e:
            print(f"   âŒ GraphRAG test failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    async def test_knowledge_retrieval(self):
        """Test knowledge retrieval and context assembly."""
        print("\nğŸ§  Testing Knowledge Retrieval")
        print("-" * 60)
        
        try:
            print("   ğŸš€ Testing knowledge retrieval...")
            
            # Create test index for knowledge retrieval
            test_index = f"knowledge-test-{uuid.uuid4().hex[:8]}"
            print(f"      ğŸ—ï¸  Creating test index: {test_index}")
            
            await self.vector_store.create_index(test_index, dimension=768)
            
            # Add knowledge content
            knowledge_content = [
                {
                    "id": "knowledge-1",
                    "content": "Machine learning is a subset of artificial intelligence that enables computers to learn from data.",
                    "metadata": {"domain": "ai", "concept": "machine_learning", "level": "basic"}
                },
                {
                    "id": "knowledge-2",
                    "content": "Deep learning uses neural networks with multiple layers to model complex patterns.",
                    "metadata": {"domain": "ai", "concept": "deep_learning", "level": "intermediate"}
                },
                {
                    "id": "knowledge-3",
                    "content": "Supervised learning uses labeled data to train models for prediction tasks.",
                    "metadata": {"domain": "ml", "concept": "supervised_learning", "level": "basic"}
                }
            ]
            
            # Generate embeddings and index
            print("      ğŸ”¤ Generating embeddings for knowledge content...")
            texts = [item["content"] for item in knowledge_content]
            embeddings = await self.embedding_service.embed_batch(texts)
            
            vectors = []
            for content, embedding in zip(knowledge_content, embeddings):
                vectors.append({
                    "id": content["id"],
                    "values": embedding,
                    "metadata": content["metadata"]
                })
            
            await self.vector_store.upsert_vectors(test_index, vectors)
            print("      âœ… Knowledge content indexed")
            
            # Wait for indexing
            await asyncio.sleep(3)
            
            # Test knowledge retrieval
            print("      ğŸ” Testing knowledge retrieval...")
            query = "What are the key concepts in AI and machine learning?"
            query_embedding = await self.embedding_service.embed_text(query)
            
            # Retrieve relevant knowledge
            knowledge_results = await self.vector_store.search(
                test_index,
                query_vector=query_embedding,
                top_k=3
            )
            
            print(f"         âœ… Knowledge retrieval returned {len(knowledge_results)} results")
            
            # Test context assembly
            print("      ğŸ§© Testing context assembly...")
            if knowledge_results:
                context_parts = []
                for result in knowledge_results:
                    if hasattr(result, 'content'):
                        context_parts.append(result.content)
                    elif hasattr(result, 'metadata') and 'text' in result.metadata:
                        context_parts.append(result.metadata['text'])
                
                assembled_context = "\n\n".join(context_parts)
                print(f"         âœ… Context assembled: {len(assembled_context)} characters")
                
                # Test LLM-based context understanding
                print("      ğŸ¤– Testing LLM context understanding...")
                try:
                    context_query = f"Based on this context, explain the key concepts:\n\n{assembled_context}"
                    llm_response = await self.llm_service.call_llm(context_query)
                    print(f"         âœ… LLM context understanding: {len(llm_response)} characters")
                except Exception as e:
                    print(f"         âš ï¸  LLM context understanding failed: {e}")
            
            # Cleanup
            await self.vector_store.delete_index(test_index)
            print("      âœ… Test index deleted")
            
            print("   âœ… Knowledge retrieval test completed")
            return True
            
        except Exception as e:
            print(f"   âŒ Knowledge retrieval test failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    async def run_all_tests(self):
        """Run all RAG service tests."""
        print("ğŸš€ Starting RAG & SEARCH SERVICES Production Readiness Test")
        print("=" * 80)
        
        # First, set up all services
        print("\nğŸ”§ PHASE 1: Service Setup")
        setup_success = await self.setup_services()
        
        if not setup_success:
            print("âŒ Service setup failed. Cannot proceed with tests.")
            return False
        
        print("\nğŸ§ª PHASE 2: Running Tests")
        tests = [
            ("RAG Engine", self.test_rag_engine),
            ("RAG Search Service", self.test_rag_search_service),
            ("Search Service", self.test_search_service),
            ("GraphRAG", self.test_graph_rag),
            ("Knowledge Retrieval", self.test_knowledge_retrieval)
        ]
        
        results = []
        for test_name, test_func in tests:
            print(f"\nğŸ§ª Running: {test_name}")
            try:
                result = await test_func()
                results.append((test_name, result))
                print(f"   {'âœ… PASSED' if result else 'âŒ FAILED'}: {test_name}")
            except Exception as e:
                print(f"   âŒ ERROR: {test_name} - {e}")
                results.append((test_name, False))
        
        # Summary
        print("\n" + "=" * 80)
        print("ğŸ“Š RAG & SEARCH SERVICES TEST SUMMARY")
        print("=" * 80)
        
        passed = sum(1 for _, result in results if result)
        total = len(results)
        
        for test_name, result in results:
            status = "âœ… PASSED" if result else "âŒ FAILED"
            print(f"{status}: {test_name}")
        
        print(f"\nğŸ¯ Overall: {passed}/{total} tests passed")
        
        if passed == total:
            print("ğŸ‰ ALL RAG TESTS PASSED! Services are production-ready!")
        else:
            print("âš ï¸  Some RAG tests failed. Check the output above for details.")
        
        return passed == total

async def main():
    """Main test function."""
    tester = RAGServicesTester()
    success = await tester.run_all_tests()
    
    if success:
        print("\nğŸ‰ RAG services test suite completed successfully!")
        sys.exit(0)
    else:
        print("\nâŒ Some RAG service tests failed!")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())

